{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ai\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MB_TO_GB_s = 0.0009765625\n",
    "PRICE_PER_GB_s = 0.0000166667\n",
    "scores = []\n",
    "RAM_LIST = list(range(128, 3072 ,64))\n",
    "resultLock = threading.Lock()\n",
    "executionResultLock = threading.Lock()\n",
    "stop_threads = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiles_files_features(s3_path, task_name):\n",
    "    compiles_ls_result = ls_aws_s3_and_capture(s3_path)\n",
    "    all_sizes = re.findall('([0-9]+) \\S*', compiles_ls_result)\n",
    "    np_compiled_sizes = np.array(list(map(int, all_sizes)))\n",
    "    file_size = int(find_file_size_s3([task_name], compiles_ls_result)[0])\n",
    "    return dyscretize_file_sizes(file_size, np_compiled_sizes, buckets = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_aws_s3_and_capture(s3_path):\n",
    "    result = ! aws s3 ls {s3_path}\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tasks_count_map(workflow_file_path):\n",
    "    with open(workflow_file_path) as json_workflow:\n",
    "        process_number_map = {}\n",
    "        data = json.load(json_workflow)\n",
    "        for process in data['processes']:\n",
    "            process_name = process['name']\n",
    "            process_number_map = append_proccess_to_map(process_number_map, process_name)\n",
    "        return process_number_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_proccess_to_map(process_number_map, process):\n",
    "    if(process in process_number_map):\n",
    "        process_number_map[process] += 1\n",
    "    else:\n",
    "        process_number_map[process] = 1\n",
    "    return process_number_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_count_features(task_name, task_map):\n",
    "    features = []\n",
    "    features.append(task_map[task_name])\n",
    "    sum_task_count = 0\n",
    "    for task in task_map.keys():\n",
    "        sum_task_count += task_map[task]\n",
    "    features.append(int(10 * task_map[task_name] / sum_task_count))\n",
    "    features.append(list(task_map.keys()).index(task_name))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_sizes_features(files_sizes, all_files_sizes):\n",
    "    size_features = []\n",
    "    size_features.append(dyscretize_file_sizes(min(files_sizes), all_files_sizes))\n",
    "    size_features.append(dyscretize_file_sizes(max(files_sizes), all_files_sizes))\n",
    "    size_features.append(dyscretize_file_sizes(sum(files_sizes)/len(files_sizes), all_files_sizes))\n",
    "    size_features.append(len(files_sizes))\n",
    "                                               \n",
    "    return size_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file_size_s3(names, output):\n",
    "    file_sizes = []\n",
    "    for name in names:\n",
    "        file_size = re.search('([0-9]+) ' + name, output)\n",
    "        file_sizes.append(file_size.group(1))\n",
    "    return file_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyscretize_file_sizes(file_size, all_files_sizes, buckets = 100):\n",
    "    all_files_bins = np.linspace(all_files_sizes.min(), all_files_sizes.max(), buckets)\n",
    "    digitized = np.digitize(file_size, bins=all_files_bins)\n",
    "    return digitized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inout_files_features(s3_path, files):\n",
    "    ls_result = ls_aws_s3_and_capture(s3_path)\n",
    "    files_sizes = []\n",
    "    for file in files:\n",
    "        file_size = re.search('([0-9]+) ' + file['name'], ls_result)\n",
    "        files_sizes.append(file_size.group(1))\n",
    "    files_sizes = list(map(int, files_sizes))\n",
    "    all_sizes = re.findall('([0-9]+) \\S*', ls_result)\n",
    "    np_sizes = np.array(list(map(int, all_sizes)))\n",
    "    return get_files_sizes_features(files_sizes, np_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_result_json():\n",
    "    init_structure = {}\n",
    "    for filename in os.listdir('./montage_0.35_single_tasks/'):\n",
    "        if(filename.endswith('.json')):\n",
    "            task_id = filename[:-5]\n",
    "            init_structure[task_id] = {}\n",
    "            for ram in RAM_LIST:\n",
    "                init_structure[task_id][ram] = []\n",
    "            init_structure[task_id]['core_features'] = []\n",
    "    with open('./montage_0.35_single_tasks/utils/results.json', 'w+') as result_json:\n",
    "        json.dump(init_structure, result_json, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_execution_result_to_json(task_id, result_dict, executionResultLock):\n",
    "    executionResultLock.acquire()\n",
    "    with open('./execution_data.json') as result_json:\n",
    "        data = json.load(result_json)\n",
    "    if 'montage_0.35' not in data:\n",
    "        data['montage_0.35'] = []\n",
    "    new_recort = {}\n",
    "    \n",
    "    new_recort['task_id'] = task_id\n",
    "    for result_key in result_dict.keys():\n",
    "        new_recort[result_key] = result_dict[result_key]\n",
    "    \n",
    "    data['montage_0.35'].append(new_recort)\n",
    "    with open('./execution_data.json', 'w+') as result_json:\n",
    "        json.dump(data, result_json, indent=4)\n",
    "    executionResultLock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_core_features(task_id, task_name, ins, outs, task_count_map, resultLock):\n",
    "    resultLock.acquire()\n",
    "    with open('./montage_0.35_single_tasks/utils/results.json') as result_json:\n",
    "        data = json.load(result_json)\n",
    "    if len(data[task_id]['core_features']) > 0:\n",
    "        resultLock.release()\n",
    "        return data[task_id]['core_features']\n",
    "        \n",
    "    all_features = get_all_features(task_name, ins, outs, task_count_map)\n",
    "    data[task_id]['core_features'] = list(map(int, all_features))\n",
    "    with open('./montage_0.35_single_tasks/utils/results.json', 'w') as result_json:\n",
    "        json.dump(data, result_json, indent=4)\n",
    "    resultLock.release()\n",
    "    return all_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(task_name, ins, outs, task_count_map):\n",
    "    all_features = []\n",
    "    all_features.extend(get_task_count_features(task_name, task_count_map))\n",
    "    all_features.extend(get_inout_files_features('cegielskir/montageV2_6-0.35/', ins))\n",
    "    all_features.extend(get_inout_files_features('cegielskir/montageV2_6-0.35/', outs))\n",
    "    all_features.extend([(get_compiles_files_features('cegielskir/montageV2_6-compiles/', task_name))])\n",
    "    return all_features             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_roundup_and_return_in_sec(time):\n",
    "    return int(math.ceil(time / 100.0)) /10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(ram, time):\n",
    "    return ram * MB_TO_GB_s * PRICE_PER_GB_s * time_roundup_and_return_in_sec(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_time_value(time, cost):\n",
    "    return (time +  35_000_000 * cost) / 40_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_task_with_brain(task_id, task_name, config, ins, outs, brain, ram, core_features, resultLock, executionResultLock):\n",
    "    resultLock.acquire()\n",
    "    with open('./montage_0.35_single_tasks/utils/results.json') as result_json:\n",
    "        data = json.load(result_json)\n",
    "    resultLock.release()\n",
    "    if data[task_id][str(ram)] == None:\n",
    "        data[task_id][str(ram)] = [] \n",
    "    if len(data[task_id][str(ram)]) == 0:\n",
    "        all_features = execute_and_save_results(data, task_id, config, ins, outs, ram, core_features[:], executionResultLock)\n",
    "        resultLock.acquire()\n",
    "        with open('./montage_0.35_single_tasks/utils/results.json') as result_json:\n",
    "            data = json.load(result_json)\n",
    "        data[task_id][str(ram)].append(all_features)\n",
    "        with open('./montage_0.35_single_tasks/utils/results.json', 'w') as result_json:\n",
    "            json.dump(data, result_json, indent=4)\n",
    "        resultLock.release()\n",
    "    else:\n",
    "        all_features = random.choice(data[task_id][str(ram)])\n",
    "        \n",
    "    old_reward = all_features[0]\n",
    "    new_ram_id = brain.update(old_reward, all_features[1:])\n",
    "    scores.append(brain.score())\n",
    "    new_correct_ram = RAM_LIST[new_ram_id]\n",
    "    new_all_features = execute_and_save_results(data, task_id, config, ins, outs, new_correct_ram, core_features[:], executionResultLock)\n",
    "\n",
    "    resultLock.acquire()\n",
    "\n",
    "    with open('./montage_0.35_single_tasks/utils/results.json') as result_json:\n",
    "        data = json.load(result_json)\n",
    "    data[task_id][str(new_correct_ram)].append(new_all_features)\n",
    "    with open('./montage_0.35_single_tasks/utils/results.json', 'w') as result_json:\n",
    "        data = json.dump(data, result_json, indent=4)\n",
    "    resultLock.release()\n",
    "\n",
    "    new_reward = new_all_features[0]\n",
    "    if old_reward == -1:\n",
    "        final_reward = new_reward * 10.0\n",
    "    else:\n",
    "        final_reward = (new_reward - old_reward) * 10.0\n",
    "        \n",
    "    if new_all_features[14] == -1:\n",
    "        final_reward = -1.0\n",
    "    elif task_name == 'mProject':\n",
    "        final_reward = final_reward * 0.09\n",
    "    elif task_name == 'mDiffFit':\n",
    "        final_reward = final_reward * 0.5\n",
    "    elif task_name == 'mBackground':\n",
    "        final_reward = final_reward * 0.43\n",
    "    elif task_name == 'mAdd':\n",
    "        final_reward = final_reward * 0.25\n",
    "    elif task_name == 'mConcatFit' or task_name == 'mImgtbl':\n",
    "        final_reward = final_reward * 0.4\n",
    "    elif task_name == 'mBgModel':\n",
    "        final_reward = final_reward * 0.2\n",
    "    elif task_name == 'mViewer':\n",
    "        final_reward = final_reward * 0.33\n",
    "\n",
    "    brain.feedback_and_reset(final_reward, new_all_features[1:])\n",
    "    print('RAM ' + str(task_id) + ' FROM: ' + str(ram) + ' --> ' + str(new_correct_ram) + ' reward: ' + str(final_reward))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [1,2,3]\n",
    "list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_and_save_results(data, task_id, config, ins, outs, ram, core_features, executionResultLock):\n",
    "    try:\n",
    "        \n",
    "        result = execute_workflow_task(config, ins, outs, ram)\n",
    "        time_dict = extract_times_from_result_string(result)\n",
    "        lambda_time = time_dict['lambda_end_time'] - time_dict['lambda_start_time']\n",
    "        download_time = time_dict['download_end_time'] - time_dict['download_start_time']\n",
    "        upload_time = time_dict['upload_end_time'] - time_dict['upload_start_time']\n",
    "        execution_time = time_dict['execution_end_time'] - time_dict['execution_start_time']\n",
    "        core_features.append(int(lambda_time/100))\n",
    "        core_features.append(int(download_time/100))\n",
    "        core_features.append(int(upload_time/100))\n",
    "        core_features.append(int(execution_time/100))\n",
    "\n",
    "        cost = calculate_cost(ram, lambda_time)    \n",
    "        core_features.append(int( 360_000 * cost))\n",
    "\n",
    "        result_json_dict = {}\n",
    "        result_json_dict['lambda_time'] = lambda_time\n",
    "        result_json_dict['download_time'] = download_time\n",
    "        result_json_dict['upload_time'] = upload_time\n",
    "        result_json_dict['execution_time'] = execution_time\n",
    "        result_json_dict['ram'] = str(ram)\n",
    "        result_json_dict['cost'] = cost\n",
    "        result_json_dict['ts'] = str(time.time())\n",
    "        add_execution_result_to_json(task_id, result_json_dict, executionResultLock)\n",
    "        core_features.insert(0,RAM_LIST.index(ram))\n",
    "        core_features.insert(0,calculate_cost_time_value(lambda_time, cost))\n",
    "    except AttributeError:\n",
    "        result_json_dict = {}\n",
    "        result_json_dict['lambda_time'] = -1\n",
    "        result_json_dict['download_time'] = -1\n",
    "        result_json_dict['upload_time'] = -1\n",
    "        result_json_dict['execution_time'] = -1\n",
    "        result_json_dict['ram'] = str(ram)\n",
    "        result_json_dict['cost'] = -1\n",
    "        result_json_dict['ts'] = str(time.time())\n",
    "        add_execution_result_to_json(task_id, result_json_dict, executionResultLock)\n",
    "        \n",
    "        core_features.append(-1)\n",
    "        core_features.append(-1)\n",
    "        core_features.append(-1)\n",
    "        core_features.append(-1)\n",
    "        core_features.append(-1)\n",
    "        core_features.insert(0,RAM_LIST.index(ram))\n",
    "        core_features.insert(0,-1)\n",
    "\n",
    "    return core_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_workflow_task(config, ins, outs, ram):\n",
    "    os.environ['FUNCTION_TYPE'] = str(ram)\n",
    "    ins_arg = json.dumps(ins)\n",
    "    outs_arg = json.dumps(outs)\n",
    "    config_arg = json.dumps(config)\n",
    "    result = !node awsLambdaCommand.js {\"'\" + ins_arg + \"'\"} {\"'\" + outs_arg + \"'\"} {\"'\" + config_arg + \"'\"}\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_times_from_result_string(result):\n",
    "    times_dict = {}\n",
    "    times_dict['download_start_time'] = int(re.search('download start: (.+?) ', result).group(1))\n",
    "    times_dict['download_end_time'] = int(re.search('download end: (.+?) ', result).group(1))\n",
    "    times_dict['upload_start_time'] = int(re.search('upload start: (.+?) ', result).group(1))\n",
    "    times_dict['upload_end_time'] = int(re.search('upload end: (.+?) ', result).group(1))\n",
    "    times_dict['execution_start_time'] = int(re.search('execution start: (.+?) ', result).group(1))\n",
    "    times_dict['execution_end_time'] = int(re.search('execution end: (.+?) ', result).group(1))\n",
    "    times_dict['lambda_start_time'] = int(re.search('lambda start: (.+?) ', result).group(1))\n",
    "    times_dict['lambda_end_time'] = int(re.search('lambda end: (.+?) ', result).group(1))\n",
    "    return times_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_ram_execution_results():\n",
    "    with open('./montage_0.35_single_tasks/utils/results.json') as result_json:\n",
    "        data = json.load(result_json)\n",
    "    for json_key in data.keys():\n",
    "        for ram in RAM_LIST:\n",
    "            if str(ram) in data[json_key]:\n",
    "                del(data[json_key][str(ram)])\n",
    "            \n",
    "        for new_ram in RAM_LIST:\n",
    "            data[json_key][str(new_ram)] = []\n",
    "\n",
    "    with open('./montage_0.35_single_tasks/utils/results.json', 'w') as result_json:\n",
    "        data = json.dump(data, result_json, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_core_features():\n",
    "    \n",
    "    task_map = get_tasks_count_map('./montage_0.35_single_tasks/utils/workflow.json')\n",
    "    with open('./montage_0.35_single_tasks/utils/results.json') as result_json:\n",
    "        data = json.load(result_json)\n",
    "    for json_key in data.keys():\n",
    "        data[json_key]['core_features'][2] = list(task_map.keys()).index(json_key.split('_')[0])\n",
    "    with open('./montage_0.35_single_tasks/utils/results.json', 'w') as result_json:\n",
    "        data = json.dump(data, result_json, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear_core_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear_ram_execution_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (abs(array - value)).argmin()\n",
    "    return array[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(brain):\n",
    "    print('Saving ...')\n",
    "    brain.save()\n",
    "    plt.plot(scores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_size_s3([task_name], s3_files_compiles.stdout)\n",
    "    print(dyscretize_file_sizes(int(exec_size[0]), np_compiled_sizes, buckets = 10))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture s3_files_compiles\n",
    "! aws s3 ls cegielskir/montageV2_6-compiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sizes = re.findall('([0-9]+) \\S*\\\\r', s3_files_compiles.stdout)\n",
    "np_sizes = np.array(list(map(int, all_sizes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = Dqn(18, len(RAM_LIST), 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(threadID, resultLock, executionResultLock):\n",
    "    #brain.load()\n",
    "    task_count_map = get_tasks_count_map('./montage_0.35_single_tasks/utils/workflow.json')\n",
    "    iteration = 0\n",
    "    while not stop_threads:\n",
    "        filename = random.choice(os.listdir('./montage_0.35_single_tasks/'))\n",
    "        if stop_threads:\n",
    "            break\n",
    "        if(filename.endswith('.json')):\n",
    "            with open('./montage_0.35_single_tasks/' + filename) as single_workflow:\n",
    "                print('Thread: ' + str (threadID) + ' iteration: ' + str(iteration))\n",
    "                iteration += 1\n",
    "                task_id = filename[:-5]\n",
    "                data = json.load(single_workflow)\n",
    "                task_name = data['processes'][0]['name']\n",
    "                process = data['processes'][0]\n",
    "                config = process['config']\n",
    "                config['workdir'] = './montage_0.35_single_tasks'\n",
    "                ins = [ data['signals'][i] for i in process['ins'] ]\n",
    "                outs = [ data['signals'][i] for i in process['outs'] ]\n",
    "                core_features = get_core_features(task_id, task_name, ins, outs, task_count_map, resultLock)\n",
    "                ram = random.choice(RAM_LIST)\n",
    "                execute_task_with_brain(task_id, task_name, config, ins, outs, brain, ram, core_features, resultLock, executionResultLock)\n",
    "\n",
    "                if iteration % 10 == 0:\n",
    "                    save(brain)\n",
    "    print('Terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultLock = threading.Lock()\n",
    "executionResultLock = threading.Lock()\n",
    "init('1', resultLock, executionResultLock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultLock = threading.Lock()\n",
    "executionResultLock = threading.Lock()\n",
    "\n",
    "#init(resultLock, executionResultLock)\n",
    "thread1 = threading.Thread(target=init, args=('1', resultLock, executionResultLock))\n",
    "thread1.start()\n",
    "thread2 = threading.Thread(target=init, args=('2',resultLock, executionResultLock))\n",
    "thread2.start()\n",
    "thread3 = threading.Thread(target=init, args=('3',resultLock, executionResultLock))\n",
    "thread3.start()\n",
    "thread4 = threading.Thread(target=init, args=('4',resultLock, executionResultLock))\n",
    "thread4.start()\n",
    "thread5 = threading.Thread(target=init, args=('5',resultLock, executionResultLock))\n",
    "thread5.start()\n",
    "thread6 = threading.Thread(target=init, args=('6',resultLock, executionResultLock))\n",
    "thread6.start()\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "thread4.join()\n",
    "thread5.join()\n",
    "thread6.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread2.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(lock):\n",
    "    lock.acquire()\n",
    "    print('cos')\n",
    "    lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_threads = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MB_TO_GB_s = 0.0009765625\n",
    "PRICE_PER_GB_s = 0.0000166667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128 * MB_TO_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1024 * MB_TO_GB * 0.0000166667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture s3_filesstate_dict\n",
    "! aws s3 ls cegielskir/montageV2_6-0.35/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture s3_files_compiles\n",
    "! aws s3 ls cegielskir/montageV2_6-compiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sizes = re.findall('([0-9]+) \\S*\\\\r', s3_files.stdout)\n",
    "np_sizes = np.array(list(map(int, all_sizes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_files_sizes = []\n",
    "for in_file in ins:\n",
    "    file_size = re.search('([0-9]+) ' + in_file['name'], s3_files.stdout)\n",
    "    ins_files_sizes.append(file_size.group(1))\n",
    "ins_files_sizes = list(map(int, ins_files_sizes))\n",
    "get_files_sizes_features(ins_files_sizes, np_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_files_sizes = []\n",
    "for out_file in outs:\n",
    "    file_size = re.search('([0-9]+) ' + out_file['name'], s3_files.stdout)\n",
    "    out_files_sizes.append(file_size.group(1))\n",
    "out_files_sizes = list(map(int, out_files_sizes))\n",
    "get_files_sizes_features(out_files_sizes, np_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sizes = re.findall('([0-9]+) \\S*\\\\r', s3_files.stdout)\n",
    "np_sizes = np.array(list(map(int, all_sizes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_series.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_bins = np.linspace(np_sizes.min(), np_sizes.max(), 20)\n",
    "digitized = np.digitize(np_sizes, bins=all_files_bins)\n",
    "digitized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(x=np_sizes, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_memory = brain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saved_memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b623e8ca54bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saved_memory' is not defined"
     ]
    }
   ],
   "source": [
    "len(saved_memory.memory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(brain):\n",
    "    print('Saving ...')\n",
    "    brain.save()\n",
    "    plt.plot(scores)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import threading\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        super(Network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100,80)\n",
    "        self.fc3 = nn.Linear(80, nb_action)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_values = self.fc3(x)\n",
    "        return q_values\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []        \n",
    "        # update the memory \n",
    "        # append the transition\n",
    "        \n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: Variable(torch.cat(x, 0)), samples)\n",
    "        \n",
    "class Dqn():\n",
    "    \n",
    "    def __init__(self, input_size, nb_action, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.reward_window = []\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        self.memory = ReplayMemory(100000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0\n",
    "        self.iter = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            probs = F.softmax(self.model(Variable(state, volatile = True))*100)\n",
    "            action = probs.multinomial(num_samples=1 )\n",
    "            self.iter = self.iter + 1\n",
    "            if self.iter % 10 == 0:\n",
    "                return torch.tensor([[1]])[0,0]\n",
    "            else:\n",
    "                return action.data[0,0]\n",
    "\n",
    "    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):\n",
    "        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "        next_outputs = self.model(batch_next_state).detach().max(1)[0]\n",
    "        target = self.gamma * next_outputs + batch_reward\n",
    "        td_loss = F.smooth_l1_loss(outputs, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def feedback_and_reset(self,reward, new_signal):\n",
    "        new_state = torch.Tensor(new_signal).float().unsqueeze(0)\n",
    "        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([reward])))\n",
    "        \n",
    "    def update(self, reward, new_signal):\n",
    "        new_state = torch.Tensor(new_signal).float().unsqueeze(0)\n",
    "        action = self.select_action(new_state)\n",
    "        if len(self.memory.memory) > 100:\n",
    "            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)\n",
    "            self.learn(batch_state, batch_next_state, batch_reward, batch_action)\n",
    "        self.last_action = action\n",
    "        self.last_state = new_state\n",
    "        self.last_reward = reward\n",
    "        self.reward_window.append(reward)\n",
    "        if len(self.reward_window) > 1000:\n",
    "            del self.reward_window[0]\n",
    "        return action\n",
    "    \n",
    "    def score(self):\n",
    "        return sum(self.reward_window)/(len(self.reward_window) +1.)\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save({'state_dict': self.model.state_dict(),\n",
    "                    'optimizer': self.optimizer.state_dict(),\n",
    "                   }, 'last_brain_2.pth')\n",
    "        \n",
    "    def load(self):\n",
    "        if os.path.isfile('./last_brain.pth'):\n",
    "            print(\"=> loading checkpoint...\")\n",
    "            checkpoint = torch.load('last_brain.pth')\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
